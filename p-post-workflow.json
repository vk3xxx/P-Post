{
  "name": "P-Post_Content_Automation",
  "comment": "Enterprise-grade workflow: Images use direct URL posting, videos use intelligent processing that elegantly handles both audio+video and video-only cases. No wait nodes - direct streaming for maximum efficiency.",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "minute",
              "expression": "*/3"
            }
          ]
        }
      },
      "id": "CronTrigger",
      "name": "CronTrigger",
      "type": "n8n-nodes-base.cron",
      "typeVersion": 1,
      "position": [-1800, 200]
    },
    {
      "parameters": {},
      "id": "ManualTrigger",
      "name": "ManualTrigger",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [-1600, 200]
    },
    {
      "parameters": {
        "keepOnlySet": true,
        "values": {
          "string": [
            {
              "name": "subreddit",
              "value": "GOONED"
            },
            {
              "name": "chat_id",
              "value": "-1002899637373"
            }
          ],
          "number": [
            {
              "name": "limit",
              "value": 30
            },
            {
              "name": "items_to_process",
              "value": 5
            },
            {
              "name": "rate_seconds",
              "value": 45
            },
            {
              "name": "memory_ttl_days",
              "value": 7
            }
          ]
        },
        "options": {}
      },
      "id": "Vars",
      "name": "Vars",
      "type": "n8n-nodes-base.set",
      "typeVersion": 1,
      "position": [-1420, 200]
    },
    {
      "parameters": {
        "url": "https://www.reddit.com/r/GOONED/new.json",
        "options": {
          "timeout": 20000
        }
      },
      "id": "FetchJSON",
      "name": "FetchJSON",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [-1230, 200]
    },
    {
      "parameters": {
        "jsCode": "function dec(u) {\n  try {\n    return u?.replace(/&amp;/g, '&');\n  } catch {\n    return u;\n  }\n}\n\nfunction largestPreview(p) {\n  const img = p?.preview?.images?.[0];\n  return dec(img?.source?.url || img?.s?.u || img?.s?.url);\n}\n\nfunction isImg(u) {\n  return typeof u === 'string' && /(\\.jpg|\\.jpeg|\\.png|\\.gif|\\.webp)(\\?|$)/i.test(u || '');\n}\n\nconst posts = items[0].json?.data?.children || [];\nconst out = [];\n\nfor (const c of posts) {\n  const p = c.data || {};\n  const base = {\n    id: p.id,\n    title: p.title,\n    permalink: `https://www.reddit.com${p.permalink}`,\n    chat_id: $node['Vars'].json['chat_id'],\n    rate_seconds: $node['Vars'].json['rate_seconds']\n  };\n\n  if (p.is_gallery && p.media_metadata) {\n    const ids = p.gallery_data?.items?.map(i => i.media_id) || Object.keys(p.media_metadata);\n    const total = ids.length;\n    ids.forEach((mid, idx) => {\n      const m = p.media_metadata[mid];\n      if (!m) return;\n      const url = dec(m?.s?.u || m?.s?.gif || m?.s?.mp4 || m?.s?.url);\n      if (!url) return;\n      out.push({\n        json: {\n          ...base,\n          is_video: false,\n          media_type: 'image',\n          media_url: url,\n          slide: idx + 1,\n          total\n        }\n      });\n    });\n    continue;\n  }\n\n  const rv = p?.secure_media?.reddit_video || p?.media?.reddit_video || p?.preview?.reddit_video_preview;\n  if (rv?.fallback_url) {\n    const v = dec(rv.fallback_url);\n    let audio = null;\n    \n    // Generate audio URL from video URL\n    try {\n      const videoUrl = new URL(v);\n      const pathParts = videoUrl.pathname.split('/');\n      const filename = pathParts[pathParts.length - 1];\n      \n      // Check if it's a DASH video file\n      if (filename.includes('DASH_') && (filename.endsWith('.mp4') || filename.endsWith('.webm'))) {\n        // Extract quality and extension\n        const match = filename.match(/DASH_(\\d+)(\\.[^.]+)$/);\n        if (match) {\n          const quality = match[1];\n          const extension = match[2];\n          \n          // Construct audio URL\n          const audioFilename = `DASH_audio${extension}`;\n          pathParts[pathParts.length - 1] = audioFilename;\n          videoUrl.pathname = pathParts.join('/');\n          audio = videoUrl.toString();\n        }\n      }\n    } catch (error) {\n      // Audio URL generation failed, continue without audio\n    }\n    \n    out.push({\n      json: {\n        ...base,\n        is_video: true,\n        media_type: 'reddit_video',\n        media_url: v,\n        audio_url: audio,\n        slide: 1,\n        total: 1\n      }\n    });\n    continue;\n  }\n\n  const direct = dec(p.url_overridden_by_dest || p.url);\n  if (isImg(direct)) {\n    out.push({\n      json: {\n        ...base,\n        is_video: false,\n        media_type: 'image',\n        media_url: direct,\n        slide: 1,\n        total: 1\n      }\n    });\n    continue;\n  }\n\n  const prev = largestPreview(p);\n  if (prev) {\n    out.push({\n      json: {\n        ...base,\n        is_video: false,\n        media_type: 'image',\n        media_url: prev,\n        slide: 1,\n        total: 1\n      }\n    });\n  }\n}\n\nreturn out;"
      },
      "id": "ExtractMedia",
      "name": "ExtractMedia",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-1040, 200]
    },
    {
      "parameters": {
        "jsCode": "// ENTERPRISE-GRADE DEDUPLICATION: Bulletproof system with multiple layers\nconst state = $getWorkflowStaticData('global');\n\n// Initialize persistent storage\nif(!state.posted) state.posted={};\nif(!state.content_fingerprints) state.content_fingerprints={};\nif(!state.url_tracking) state.url_tracking={};\nif(!state.post_history) state.post_history=[];\n\n// Clean up old entries with configurable TTL\nconst ttl=($node['Vars'].json['memory_ttl_days']||7)*24*60*60*1000;\nconst now=Date.now();\n\n// Cleanup old entries\nfor(const [k,t] of Object.entries(state.posted)){\n  if(!t||now-t>ttl) delete state.posted[k];\n}\nfor(const [k,f] of Object.entries(state.content_fingerprints)){\n  if(!f||now-f.timestamp>ttl) delete state.content_fingerprints[k];\n}\nfor(const [k,u] of Object.entries(state.url_tracking)){\n  if(!u||now-u>ttl) delete state.url_tracking[k];\n}\n\n// Keep only recent post history\nstate.post_history = state.post_history.filter(h => now - h.timestamp < ttl);\n\nconst fresh=[];\nconst duplicates=[];\n\nfor(const it of items){\n  const item = it.json;\n  \n  // ENTERPRISE DEDUPLICATION STRATEGY 1: Enhanced Key\n  let dedupKey = item.deduplication_key;\n  if(!dedupKey) dedupKey = `${item.id}_${item.media_type}_${item.slide}`;\n  if(!dedupKey) dedupKey = item.id;\n  \n  // ENTERPRISE DEDUPLICATION STRATEGY 2: Content Fingerprint\n  const contentFingerprint = {\n    reddit_post_id: item.id,\n    title: item.title,\n    permalink: item.permalink,\n    media_type: item.media_type,\n    slide: item.slide,\n    media_url_hash: item.media_url ? item.media_url.split('/').pop().split('?')[0] : null\n  };\n  const fingerprintKey = JSON.stringify(contentFingerprint);\n  \n  // ENTERPRISE DEDUPLICATION STRATEGY 3: URL Tracking\n  const urlKey = item.media_url ? item.media_url.split('/').pop().split('?')[0] : null;\n  \n  // ENTERPRISE DEDUPLICATION STRATEGY 4: Reddit Post ID + Media Type\n  const postKey = `post_${item.id}_${item.media_type}`;\n  \n  // MULTI-LAYER DEDUPLICATION CHECK\n  const isDuplicateByKey = state.posted[dedupKey];\n  const isDuplicateByFingerprint = state.content_fingerprints[fingerprintKey];\n  const isDuplicateByURL = urlKey && state.url_tracking[urlKey];\n  const isDuplicateByPost = state.posted[postKey];\n  \n  // Check if ANY deduplication method detects a duplicate\n  const isDuplicate = isDuplicateByKey || isDuplicateByFingerprint || isDuplicateByURL || isDuplicateByPost;\n  \n  if(!isDuplicate) {\n    console.log('✅ ENTERPRISE DEDUP: NEW CONTENT - Adding to fresh list');\n    console.log('  Key:', dedupKey);\n    console.log('  Fingerprint:', contentFingerprint);\n    console.log('  URL Hash:', urlKey);\n    console.log('  Post Key:', postKey);\n    \n    // Store in ALL tracking systems for maximum reliability\n    state.posted[dedupKey] = now;\n    state.content_fingerprints[fingerprintKey] = {\n      ...contentFingerprint,\n      dedup_key: dedupKey,\n      timestamp: now,\n      tracked_at: new Date(now).toISOString()\n    };\n    if(urlKey) state.url_tracking[urlKey] = now;\n    state.posted[postKey] = now;\n    \n    // Add to post history for audit trail\n    state.post_history.push({\n      id: item.id,\n      media_type: item.media_type,\n      slide: item.slide,\n      title: item.title,\n      permalink: item.permalink,\n      media_url: item.media_url,\n      dedup_key: dedupKey,\n      timestamp: now,\n      posted_at: new Date(now).toISOString()\n    });\n    \n    fresh.push(it);\n  } else {\n    console.log('🚫 ENTERPRISE DEDUP: DUPLICATE DETECTED - Skipping content');\n    console.log('  Item ID:', item.id);\n    console.log('  Media Type:', item.media_type);\n    console.log('  Slide:', item.slide);\n    console.log('  Title:', item.title);\n    \n    if(isDuplicateByKey) console.log('  Duplicate by key:', dedupKey, 'Last posted:', new Date(state.posted[dedupKey]).toISOString());\n    if(isDuplicateByFingerprint) console.log('  Duplicate by fingerprint:', new Date(state.content_fingerprints[fingerprintKey].timestamp).toISOString());\n    if(isDuplicateByURL) console.log('  Duplicate by URL:', urlKey, 'Last posted:', new Date(state.url_tracking[urlKey]).toISOString());\n    if(isDuplicateByPost) console.log('  Duplicate by post:', postKey, 'Last posted:', new Date(state.posted[postKey]).toISOString());\n    \n    duplicates.push({\n      item: item,\n      reason: {\n        byKey: isDuplicateByKey,\n        byFingerprint: isDuplicateByFingerprint,\n        byURL: isDuplicateByURL,\n        byPost: isDuplicateByPost\n      }\n    });\n  }\n}\n\n// ENTERPRISE LOGGING AND MONITORING\nconsole.log(`\\n🎯 ENTERPRISE DEDUPLICATION SUMMARY:`);\nconsole.log(`  Input Items: ${items.length}`);\nconsole.log(`  Fresh Items: ${fresh.length}`);\nconsole.log(`  Duplicates Blocked: ${duplicates.length}`);\nconsole.log(`  Active Keys: ${Object.keys(state.posted).length}`);\nconsole.log(`  Active Fingerprints: ${Object.keys(state.content_fingerprints).length}`);\nconsole.log(`  Active URLs: ${Object.keys(state.url_tracking).length}`);\nconsole.log(`  Post History: ${state.post_history.length} entries`);\nconsole.log(`  Memory TTL: ${($node['Vars'].json['memory_ttl_days']||7)} days\\n`);\n\nreturn fresh;"
      },
      "id": "Deduplicate",
      "name": "Deduplicate",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-820, 200]
    },
    {
      "parameters": {
        "jsCode": "// LOOP CONTROL: Process specified number of items with iteration tracking\nconst state = $getWorkflowStaticData('global');\nconst itemsToProcess = $node['Vars'].json['items_to_process'] || 5;\n\n// Initialize loop state if not exists\nif(!state.loop_iteration) state.loop_iteration = 0;\nif(!state.loop_start_time) state.loop_start_time = Date.now();\nif(!state.total_items_processed) state.total_items_processed = 0;\nif(!state.loop_items) state.loop_items = [];\n\n// Get fresh items from deduplication\nconst freshItems = items;\nconsole.log(`\\n🔄 LOOP CONTROL: Starting iteration ${state.loop_iteration + 1}/${itemsToProcess}`);\nconsole.log(`  Fresh items available: ${freshItems.length}`);\nconsole.log(`  Items to process: ${itemsToProcess}`);\nconsole.log(`  Total processed so far: ${state.total_items_processed}`);\n\n// Add fresh items to loop queue if not already there\nfor(const item of freshItems) {\n  const itemKey = `${item.json.id}_${item.json.media_type}_${item.json.slide}`;\n  const alreadyQueued = state.loop_items.some(queued => \n    `${queued.json.id}_${queued.json.media_type}_${queued.json.slide}` === itemKey\n  );\n  \n  if(!alreadyQueued) {\n    state.loop_items.push(item);\n    console.log(`  Added to loop queue: ${item.json.title} (${item.json.media_type})`);\n  }\n}\n\n// Determine how many items to process in this iteration\nconst itemsInQueue = state.loop_items.length;\nconst itemsToProcessThisIteration = Math.min(itemsToProcess - state.total_items_processed, itemsInQueue);\n\nconsole.log(`  Items in queue: ${itemsInQueue}`);\nconsole.log(`  Items to process this iteration: ${itemsToProcessThisIteration}`);\n\nif(itemsToProcessThisIteration <= 0) {\n  console.log(`✅ LOOP COMPLETE: All ${itemsToProcess} items have been processed`);\n  console.log(`  Total items processed: ${state.total_items_processed}`);\n  console.log(`  Loop duration: ${((Date.now() - state.loop_start_time) / 1000).toFixed(2)} seconds`);\n  \n  // Reset loop state for next workflow run\n  state.loop_iteration = 0;\n  state.loop_start_time = Date.now();\n  state.total_items_processed = 0;\n  state.loop_items = [];\n  \n  return [];\n}\n\n// Take items for this iteration\nconst itemsForThisIteration = state.loop_items.splice(0, itemsToProcessThisIteration);\nstate.total_items_processed += itemsForThisIteration;\nstate.loop_iteration++;\n\nconsole.log(`  Processing ${itemsForThisIteration.length} items in iteration ${state.loop_iteration}`);\nfor(const item of itemsForThisIteration) {\n  console.log(`    - ${item.json.title} (${item.json.media_type})`);\n}\n\nconsole.log(`  Progress: ${state.total_items_processed}/${itemsToProcess} (${((state.total_items_processed/itemsToProcess)*100).toFixed(1)}%)`);\nconsole.log(`  Remaining in queue: ${state.loop_items.length}`);\n\nreturn itemsForThisIteration;"
      },
      "id": "LoopControl",
      "name": "LoopControl",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-700, 200]
    },

    {
      "parameters": {
        "jsCode": "// Persistent deduplication with file backup\nconst item = $input.item;\nconsole.log('=== PERSISTENT DEDUPLICATION ===');\n\n// Get the original data from ExtractMedia\nconst originalData = $('ExtractMedia').item.json;\n\nif (!originalData) {\n  console.log('ERROR: Could not access original data from ExtractMedia');\n  throw new Error('Could not access original data from ExtractMedia');\n}\n\n// Create a unique content fingerprint\nconst contentFingerprint = {\n  reddit_post_id: originalData.id,\n  media_type: originalData.media_type,\n  slide: originalData.slide,\n  title: originalData.title,\n  permalink: originalData.permalink,\n  timestamp: Date.now()\n};\n\nconsole.log('Content fingerprint created:', contentFingerprint);\n\n// Pass through the data with enhanced deduplication info\nreturn [{\n  json: {\n    ...item.json,\n    content_fingerprint: JSON.stringify(contentFingerprint),\n    deduplication_key: `${originalData.id}_${originalData.media_type}_${originalData.slide}`,\n    is_unique_content: true\n  }\n}];"
      },
      "id": "PersistentDeduplicate",
      "name": "PersistentDeduplicate",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-930, 200]
    },
    {
      "parameters": {
        "jsCode": "return items.map(it => {\n  const j = it.json;\n  j.caption = j.total > 1 ? `${j.title} [${j.slide}/${j.total}]\\n${j.permalink}` : `${j.title}\\n${j.permalink}`;\n  return { json: j };\n});"
      },
      "id": "CaptionBuilder",
      "name": "CaptionBuilder",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-620, 200]
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{$json.is_video}}",
              "value2": true
            }
          ]
        }
      },
      "id": "IfVideo",
      "name": "IfVideo",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [-420, 200]
    },
    {
      "parameters": {
        "batchSize": 10,
        "options": {}
      },
      "id": "SplitImages",
      "name": "SplitImages",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 1,
      "position": [-220, 60]
    },
    {
      "parameters": {
        "jsCode": "// ENTERPRISE IMAGE DEDUPLICATION: Final checkpoint for images\nconst state = $getWorkflowStaticData('global');\n\n// Initialize if not exists\nif(!state.image_tracking) state.image_tracking={};\nif(!state.image_posted) state.image_posted={};\n\nconst now=Date.now();\nconst ttl=($node['Vars'].json['memory_ttl_days']||7)*24*60*60*1000;\n\n// Cleanup old entries\nfor(const [k,t] of Object.entries(state.image_tracking)){\n  if(!t||now-t>ttl) delete state.image_tracking[k];\n}\nfor(const [k,p] of Object.entries(state.image_posted)){\n  if(!p||now-p>ttl) delete state.image_posted[k];\n}\n\nconst fresh=[];\nconst blocked=[];\n\nfor(const it of items){\n  const item = it.json;\n  \n  // MULTIPLE DEDUPLICATION STRATEGIES FOR IMAGES\n  const imageKey1 = `${item.id}_${item.media_type}_${item.slide}`;\n  const imageKey2 = item.media_url ? item.media_url.split('/').pop().split('?')[0] : null;\n  const imageKey3 = `img_${item.id}_${item.media_type}`;\n  const imageKey4 = item.deduplication_key;\n  \n  // Check ALL deduplication methods\n  const isDuplicate1 = state.image_tracking[imageKey1];\n  const isDuplicate2 = imageKey2 && state.image_tracking[imageKey2];\n  const isDuplicate3 = state.image_tracking[imageKey3];\n  const isDuplicate4 = imageKey4 && state.image_tracking[imageKey4];\n  \n  // Also check main deduplication system\n  const mainDedupKey = `${item.id}_${item.media_type}_${item.slide}`;\n  const isDuplicateMain = state.posted && state.posted[mainDedupKey];\n  \n  const isDuplicate = isDuplicate1 || isDuplicate2 || isDuplicate3 || isDuplicate4 || isDuplicateMain;\n  \n  if(!isDuplicate) {\n    console.log('✅ IMAGE DEDUP: NEW IMAGE - Adding to fresh list');\n    console.log('  ID:', item.id);\n    console.log('  Media Type:', item.media_type);\n    console.log('  Slide:', item.slide);\n    console.log('  Title:', item.title);\n    console.log('  URL Hash:', imageKey2);\n    \n    // Store in ALL image tracking systems\n    state.image_tracking[imageKey1] = now;\n    if(imageKey2) state.image_tracking[imageKey2] = now;\n    state.image_tracking[imageKey3] = now;\n    if(imageKey4) state.image_tracking[imageKey4] = now;\n    \n    // Mark as posted\n    state.image_posted[imageKey1] = now;\n    \n    fresh.push(it);\n  } else {\n    console.log('🚫 IMAGE DEDUP: DUPLICATE IMAGE BLOCKED');\n    console.log('  ID:', item.id);\n    console.log('  Media Type:', item.media_type);\n    console.log('  Slide:', item.slide);\n    console.log('  Title:', item.title);\n    \n    if(isDuplicate1) console.log('  Duplicate by key1:', imageKey1, 'Last posted:', new Date(state.image_tracking[imageKey1]).toISOString());\n    if(isDuplicate2) console.log('  Duplicate by key2:', imageKey2, 'Last posted:', new Date(state.image_tracking[imageKey2]).toISOString());\n    if(isDuplicate3) console.log('  Duplicate by key3:', imageKey3, 'Last posted:', new Date(state.image_tracking[imageKey3]).toISOString());\n    if(isDuplicate4) console.log('  Duplicate by key4:', imageKey4, 'Last posted:', new Date(state.image_tracking[imageKey4]).toISOString());\n    if(isDuplicateMain) console.log('  Duplicate by main system:', mainDedupKey, 'Last posted:', new Date(state.posted[mainDedupKey]).toISOString());\n    \n    blocked.push({\n      item: item,\n      reason: {\n        byKey1: isDuplicate1,\n        byKey2: isDuplicate2,\n        byKey3: isDuplicate3,\n        byKey4: isDuplicate4,\n        byMain: isDuplicateMain\n      }\n    });\n  }\n}\n\nconsole.log(`\\n🖼️ IMAGE DEDUPLICATION SUMMARY:`);\nconsole.log(`  Input Images: ${items.length}`);\nconsole.log(`  Fresh Images: ${fresh.length}`);\nconsole.log(`  Blocked Images: ${blocked.length}`);\nconsole.log(`  Active Image Keys: ${Object.keys(state.image_tracking).length}`);\nconsole.log(`  Posted Images: ${Object.keys(state.image_posted).length}\\n`);\n\nreturn fresh;"
      },
      "id": "ImageDeduplicate",
      "name": "ImageDeduplicate",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [-100, 60]
    },
    {
      "parameters": {
        "operation": "sendPhoto",
        "chatId": "={{$json.chat_id}}",
        "file": "={{ $json.media_url }}",
        "additionalFields": {
          "caption": "={{$json.caption}}"
        }
      },
      "id": "SendPhoto",
      "name": "SendPhoto",
      "type": "n8n-nodes-base.telegram",
      "typeVersion": 1,
      "position": [0, 60]
    },
    {
      "parameters": {
        "batchSize": 10,
        "options": {}
      },
      "id": "SplitVideos",
      "name": "SplitVideos",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 1,
      "position": [-220, 340]
    },

    {
      "parameters": {
        "jsCode": "// Debug what DLVideo is trying to download\nconst item = $input.item;\nconsole.log('=== DEBUG DLVIDEO ===');\nconsole.log('Input item to DebugDLVideo:', JSON.stringify(item.json, null, 2));\nconsole.log('Item keys:', Object.keys(item.json || {}));\nconsole.log('Media URL:', item.json?.media_url);\nconsole.log('Media type:', item.json?.media_type);\nconsole.log('ID:', item.json?.id);\nconsole.log('Slide:', item.json?.slide);\nconsole.log('Chat ID:', item.json?.chat_id);\nconsole.log('Caption:', item.json?.caption);\n\n// Check if we have the required fields\nif (!item.json?.id || !item.json?.media_type || !item.json?.slide || !item.json?.media_url || !item.json?.chat_id || !item.json?.caption) {\n  console.log('ERROR: Missing required fields in DebugDLVideo input');\n  console.log('id:', item.json?.id);\n  console.log('media_type:', item.json?.media_type);\n  console.log('slide:', item.json?.slide);\n  console.log('media_url:', item.json?.media_url);\n  console.log('chat_id:', item.json?.chat_id);\n  console.log('caption:', item.json?.caption);\n  throw new Error('Missing required fields in DebugDLVideo input');\n}\n\n// Check if we have a valid media URL\nif (!item.json.media_url) {\n  console.log('ERROR: No media_url provided');\n  throw new Error('No media_url provided for video download');\n}\n\n// Check if the URL looks valid\nif (!item.json.media_url.startsWith('http')) {\n  console.log('ERROR: Invalid media_url format:', item.json.media_url);\n  throw new Error('Invalid media_url format');\n}\n\nconsole.log('DLVideo should download from:', item.json.media_url);\n\n// Pass through the data\nreturn [{\n  json: {\n    id: item.json.id,\n    media_type: item.json.media_type,\n    slide: item.json.slide,\n    chat_id: item.json.chat_id,\n    caption: item.json.caption,\n    media_url: item.json.media_url\n  }\n}];"
      },
      "id": "DebugDLVideo",
      "name": "DebugDLVideo",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [150, 300]
    },
    {
      "parameters": {
        "jsCode": "// Reddit API-based download strategy\nconst item = $input.item;\nconsole.log('=== REDDIT API STRATEGY ===');\nconsole.log('Input item to SmartDownload:', JSON.stringify(item.json, null, 2));\n\n// Check if we have the required fields\nif (!item.json.id || !item.json.media_type || !item.json.slide || !item.json.media_url || !item.json.caption) {\n  console.log('ERROR: Missing required fields in SmartDownload input');\n  console.log('id:', item.json.id);\n  console.log('media_type:', item.json.media_type);\n  console.log('slide:', item.json.slide);\n  console.log('media_url:', item.json.media_url);\n  console.log('caption:', item.json.caption);\n  throw new Error('Missing required fields in SmartDownload input');\n}\n\n// Extract the post ID from the caption URL\nconst caption = item.json.caption || '';\nconst redditUrlMatch = caption.match(/https:\\/\\/www\\.reddit\\.com\\/r\\/[^\\/]+\\/comments\\/([^\\/]+)\\/[^\\/]+\\//);\n\nif (!redditUrlMatch) {\n  console.log('ERROR: Could not extract Reddit post ID from caption');\n  console.log('Caption:', caption);\n  throw new Error('Could not extract Reddit post ID from caption');\n}\n\nconst postId = redditUrlMatch[1];\nconsole.log('Reddit post ID extracted:', postId);\n\n// Use Reddit's JSON API to get video data\nconst redditApiUrl = `https://www.reddit.com/api/info.json?id=t3_${postId}`;\nconsole.log('Reddit API URL:', redditApiUrl);\n\nconsole.log('Using Reddit API approach for post:', postId);\n\n// Pass through the data with API information\nreturn [{\n  json: {\n    id: item.json.id,\n    media_type: item.json.media_type,\n    slide: item.json.slide,\n    chat_id: item.json.chat_id,\n    caption: item.json.caption,\n    media_url: item.json.media_url,\n    reddit_post_id: postId,\n    reddit_api_url: redditApiUrl,\n    use_reddit_api: true\n  }\n}];"
      },
      "id": "SmartDownload",
      "name": "SmartDownload",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [175, 300]
    },
    {
      "parameters": {
        "url": "={{$json.reddit_api_url}}",
        "responseFormat": "json",
        "options": {
          "timeout": 30000,
          "headers": {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json",
            "Accept-Language": "en-US,en;q=0.9"
          }
        }
      },
      "id": "RedditAPI",
      "name": "RedditAPI",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [200, 300]
    },
    {
      "parameters": {
        "jsCode": "// Process Reddit API response to extract actual video URL\nconst item = $input.item;\nconsole.log('=== PROCESSING REDDIT API RESPONSE ===');\n\n// Get the Reddit API response\nconst redditData = item.json;\nconsole.log('Reddit API response received');\n\n// Extract the original item data from the workflow context\nconst originalData = $('SmartDownload').item.json;\n\nif (!originalData) {\n  console.log('ERROR: Could not access original data from SmartDownload');\n  throw new Error('Could not access original data from SmartDownload');\n}\n\nconsole.log('Original data retrieved:', {\n  id: originalData.id,\n  media_type: originalData.media_type,\n  reddit_post_id: originalData.reddit_post_id\n});\n\n// Parse the Reddit API response to find the actual video URL\nlet actualVideoUrl = originalData.media_url;\n\nif (redditData && redditData.data && redditData.data.children && redditData.data.children.length > 0) {\n  const postData = redditData.data.children[0].data;\n  console.log('Post data from Reddit API:', {\n    title: postData.title,\n    url: postData.url,\n    media: postData.media ? 'Has media' : 'No media',\n    secure_media: postData.secure_media ? 'Has secure media' : 'No secure media'\n  });\n  \n  // Try to extract video URL from the API response\n  if (postData.secure_media && postData.secure_media.reddit_video) {\n    const redditVideo = postData.secure_media.reddit_video;\n    if (redditVideo.fallback_url) {\n      actualVideoUrl = redditVideo.fallback_url;\n      console.log('Found video URL from API:', actualVideoUrl);\n    }\n  } else if (postData.media && postData.media.reddit_video) {\n    const redditVideo = postData.media.reddit_video;\n    if (redditVideo.fallback_url) {\n      actualVideoUrl = redditVideo.fallback_url;\n      console.log('Found video URL from API:', actualVideoUrl);\n    }\n  }\n} else {\n  console.log('No valid post data found in API response');\n}\n\nconsole.log('Final video URL to download:', actualVideoUrl);\n\n// Pass through the data with the actual video URL\nreturn [{\n  json: {\n    id: originalData.id,\n    media_type: originalData.media_type,\n    slide: originalData.slide,\n    chat_id: originalData.chat_id,\n    caption: originalData.caption,\n    media_url: actualVideoUrl,\n    reddit_post_id: originalData.reddit_post_id,\n    reddit_api_data: redditData,\n    use_reddit_api: true\n  }\n}];"
      },
      "id": "ProcessRedditAPI",
      "name": "ProcessRedditAPI",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [225, 300]
    },
    {
      "parameters": {
        "jsCode": "// POST-API DEDUPLICATION: Catch duplicates after Reddit API URL changes\nconst state = $getWorkflowStaticData('global');\nif(!state.posted) state.posted={};\nif(!state.content_fingerprints) state.content_fingerprints={};\n\nconst fresh=[];\nfor(const it of items){\n  const item = it.json;\n  \n  // Create a robust deduplication key that survives URL changes\n  const dedupKey = `${item.id}_${item.media_type}_${item.slide}`;\n  \n  // Create content fingerprint for additional deduplication\n  const contentFingerprint = {\n    reddit_post_id: item.id,\n    title: item.title,\n    permalink: item.permalink,\n    media_type: item.media_type,\n    slide: item.slide\n  };\n  \n  const fingerprintKey = JSON.stringify(contentFingerprint);\n  \n  // Check both deduplication methods\n  const isDuplicateByKey = state.posted[dedupKey];\n  const isDuplicateByFingerprint = state.content_fingerprints[fingerprintKey];\n  \n  if(!isDuplicateByKey && !isDuplicateByFingerprint) {\n    console.log('POST-API: NEW CONTENT - Adding to fresh list with key:', dedupKey);\n    console.log('Content fingerprint:', contentFingerprint);\n    \n    // Store both deduplication methods\n    state.posted[dedupKey] = Date.now();\n    state.content_fingerprints[fingerprintKey] = {\n      ...contentFingerprint,\n      dedup_key: dedupKey,\n      timestamp: Date.now()\n    };\n    \n    fresh.push(it);\n  } else {\n    if(isDuplicateByKey) {\n      console.log('POST-API: DUPLICATE DETECTED by key:', dedupKey, 'Last posted:', new Date(state.posted[dedupKey]));\n    }\n    if(isDuplicateByFingerprint) {\n      console.log('POST-API: DUPLICATE DETECTED by fingerprint:', fingerprintKey, 'Last posted:', new Date(state.content_fingerprints[fingerprintKey].timestamp));\n    }\n  }\n}\n\nconsole.log(`POST-API Deduplication: ${items.length} input items, ${fresh.length} fresh items`);\nreturn fresh;"
      },
      "id": "PostAPIDeduplicate",
      "name": "PostAPIDeduplicate",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [250, 300]
    },
    {
      "parameters": {
        "command": "={{$json.resolved_curl_command}}"
      },
      "id": "DLVideo",
      "name": "DLVideo",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [275, 300]
    },

    {
      "parameters": {
        "command": "={{$json.resolved_audio_curl_command}}"
      },
      "id": "DLAudio",
      "name": "DLAudio",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [200, 400]
    },



    {
      "parameters": {
        "jsCode": "// Build the curl command with resolved values\nconst item = $input.item;\nconsole.log('=== BUILDING CURL COMMAND ===');\nconsole.log('Input item:', JSON.stringify(item.json, null, 2));\n\nconst { id, media_type, slide, media_url, chat_id, caption } = item.json;\nconsole.log('Building command for:', { id, media_type, slide, media_url });\n\n// Validate required fields\nif (!id || !media_type || !slide || !media_url) {\n  console.log('ERROR: Missing required fields in BuildCurlCommand');\n  console.log('id:', id);\n  console.log('media_type:', media_type);\n  console.log('slide:', slide);\n  console.log('media_url:', media_url);\n  throw new Error('Missing required fields in BuildCurlCommand');\n}\n\n// Build the complete curl command with resolved values and better debugging\nconst curlCommand = `curl -L -v -o /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 -H 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36' -H 'Referer: https://www.reddit.com/' -H 'Accept: */*' -H 'Accept-Language: en-US,en;q=0.9' -H 'Accept-Encoding: gzip, deflate, br' -H 'Sec-Fetch-Dest: video' -H 'Sec-Fetch-Mode: no-cors' -H 'Sec-Fetch-Site: cross-site' -H 'Cache-Control: no-cache' -H 'Pragma: no-cache' --connect-timeout 30 --max-time 120 --write-out 'HTTP_CODE:%{http_code}\\nSIZE_DOWNLOADED:%{size_downloaded}\\nTIME_TOTAL:%{time_total}\\n' '${media_url}' && echo '=== DOWNLOAD COMPLETED ===' && ls -lh /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 && echo '=== FILE VERIFICATION ==='`;\n\nconsole.log('Built curl command:', curlCommand);\n\n// Pass through the data with the resolved command\nreturn [{\n  json: {\n    id,\n    media_type,\n    slide,\n    chat_id,\n    caption,\n    media_url,\n    reddit_post_id: item.json.reddit_post_id,\n    reddit_api_data: item.json.reddit_api_data,\n    use_reddit_api: true,\n    resolved_curl_command: curlCommand\n  }\n}];"
      },
      "id": "BuildCurlCommand",
      "name": "BuildCurlCommand",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [250, 300]
    },

    {
      "parameters": {
        "jsCode": "// Build the audio curl command with resolved values\nconst item = $input.item;\nconsole.log('=== BUILDING AUDIO CURL COMMAND ===');\nconsole.log('Input item:', JSON.stringify(item.json, null, 2));\n\nconst { id, media_type, slide, audio_url, chat_id, caption } = item.json;\nconsole.log('Building audio command for:', { id, media_type, slide, audio_url });\n\n// Check if audio URL exists\nif (!audio_url) {\n  console.log('No audio URL found, skipping audio download');\n  return [{\n    json: {\n      id,\n      media_type,\n      slide,\n      chat_id,\n      caption,\n      media_url: item.json.media_url,\n      audio_url: null,\n      has_audio: false,\n      resolved_audio_curl_command: \"echo \\\"No audio to download\\\"\"\n    }\n  }];\n}\n\n// Validate required fields\nif (!id || !media_type || !slide || !audio_url) {\n  console.log('ERROR: Missing required fields in BuildAudioCommand');\n  console.log('id:', id);\n  console.log('media_type:', media_type);\n  console.log('slide:', slide);\n  console.log('audio_url:', audio_url);\n  throw new Error('Missing required fields in BuildAudioCommand');\n}\n\n// Build the complete audio curl command with resolved values and better debugging\nconst audioCurlCommand = `curl -L -v -o /tmp/n8n_audio_${id}_${media_type}_${slide}.mp4 -H 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36' -H 'Referer: https://www.reddit.com/' -H 'Accept: */*' -H 'Accept-Language: en-US,en;q=0.9' -H 'Accept-Encoding: gzip, deflate, br' -H 'Sec-Fetch-Dest: audio' -H 'Sec-Fetch-Mode: no-cors' -H 'Sec-Fetch-Site: cross-site' -H 'Cache-Control: no-cache' -H 'Pragma: no-cache' --connect-timeout 30 --max-time 120 --write-out 'HTTP_CODE:%{http_code}\\nSIZE_DOWNLOADED:%{size_downloaded}\\nTIME_TOTAL:%{time_total}\\n' '${audio_url}' && echo '=== AUDIO DOWNLOAD COMPLETED ===' && ls -lh /tmp/n8n_audio_${id}_${media_type}_${slide}.mp4 && echo '=== AUDIO FILE VERIFICATION ==='`;\n\nconsole.log('Built audio curl command:', audioCurlCommand);\n\n// Pass through the data with the resolved command\nreturn [{\n  json: {\n    id,\n    media_type,\n    slide,\n    chat_id,\n    caption,\n    media_url: item.json.media_url,\n    audio_url,\n    has_audio: true,\n    reddit_post_id: item.json.reddit_post_id,\n    reddit_api_data: item.json.reddit_api_data,\n    use_reddit_api: true,\n    resolved_audio_curl_command: audioCurlCommand\n  }\n}];"
      },
      "id": "BuildAudioCommand",
      "name": "BuildAudioCommand",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [250, 400]
    },
    {
      "parameters": {
        "jsCode": "// Preserve video data after DLVideo download\nconst item = $input.item;\nconsole.log('=== PRESERVE VIDEO DATA ===');\nconsole.log('Input item:', JSON.stringify(item.json, null, 2));\n\n// Get the original data from the workflow context (before DLVideo)\nconst originalData = $('ProcessRedditAPI').item.json;\n\nif (!originalData) {\n  console.log('ERROR: Could not access original data from ProcessRedditAPI');\n  throw new Error('Could not access original data from ProcessRedditAPI');\n}\n\nconsole.log('Original data retrieved:', {\n  id: originalData.id,\n  media_type: originalData.media_type,\n  slide: originalData.slide,\n  chat_id: originalData.chat_id,\n  caption: originalData.caption,\n  media_url: originalData.media_url\n});\n\n// Pass through the data with all required fields preserved\nreturn [{\n  json: {\n    id: originalData.id,\n    media_type: originalData.media_type,\n    slide: originalData.slide,\n    chat_id: originalData.chat_id,\n    caption: originalData.caption,\n    media_url: originalData.media_url,\n    download_completed: true,\n    file_path: `/tmp/n8n_video_${originalData.id}_${originalData.media_type}_${originalData.slide}.mp4`\n  }\n}];"
      },
      "id": "PreserveVideoData",
      "name": "PreserveVideoData",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [450, 300]
    },

    {
      "parameters": {
        "jsCode": "// Preserve audio data after DLAudio download\nconst item = $input.item;\nconsole.log('=== PRESERVE AUDIO DATA ===');\nconsole.log('Input item:', JSON.stringify(item.json, null, 2));\n\n// Get the original data from the workflow context (before DLAudio)\nconst originalData = $('ProcessRedditAPI').item.json;\n\nif (!originalData) {\n  console.log('ERROR: Could not access original data from ProcessRedditAPI');\n  throw new Error('Could not access original data from ProcessRedditAPI');\n}\n\n// Check if audio URL exists\nif (!originalData.audio_url) {\n  console.log('No audio URL found, skipping audio processing');\n  return [{\n    json: {\n      id: originalData.id,\n      media_type: originalData.media_type,\n      slide: originalData.slide,\n      chat_id: originalData.chat_id,\n      caption: originalData.caption,\n      media_url: originalData.media_url,\n      audio_url: null,\n      has_audio: false,\n      audio_download_completed: false,\n      audio_file_path: null\n    }\n  }];\n}\n\nconsole.log('Audio URL found:', originalData.audio_url);\nconsole.log('Original data retrieved:', {\n  id: originalData.id,\n  media_type: originalData.media_type,\n  slide: originalData.slide,\n  chat_id: originalData.chat_id,\n  caption: originalData.caption,\n  audio_url: originalData.audio_url\n});\n\n// Pass through the data with all required fields preserved\nreturn [{\n  json: {\n    id: originalData.id,\n    media_type: originalData.media_type,\n    slide: originalData.slide,\n    chat_id: originalData.chat_id,\n    caption: originalData.caption,\n    media_url: originalData.media_url,\n    audio_url: originalData.audio_url,\n    has_audio: true,\n    audio_download_completed: true,\n    audio_file_path: `/tmp/n8n_audio_${originalData.id}_${originalData.media_type}_${originalData.slide}.mp4`\n  }\n}];"
      },
      "id": "PreserveAudioData",
      "name": "PreserveAudioData",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [450, 400]
    },

    {
      "parameters": {
        "jsCode": "// Merge video and audio data for processing\nconst item = $input.item;\nconsole.log('=== MERGE VIDEO AUDIO DATA ===');\nconsole.log('Input item:', JSON.stringify(item.json, null, 2));\n\n// Get the original data from the workflow context\nconst originalData = $('ProcessRedditAPI').item.json;\n\nif (!originalData) {\n  console.log('ERROR: Could not access original data from ProcessRedditAPI');\n  throw new Error('Could not access original data from ProcessRedditAPI');\n}\n\n// Check if we have both video and audio data\nconst hasVideo = item.json.download_completed || item.json.audio_download_completed;\nconst hasAudio = item.json.has_audio && item.json.audio_download_completed;\n\nconsole.log('Data status:', { hasVideo, hasAudio });\n\n// Pass through the data with all required fields preserved\nreturn [{\n  json: {\n    id: originalData.id,\n    media_type: originalData.media_type,\n    slide: originalData.slide,\n    chat_id: originalData.chat_id,\n    caption: originalData.caption,\n    media_url: originalData.media_url,\n    audio_url: originalData.audio_url,\n    has_audio: hasAudio,\n    download_completed: hasVideo,\n    audio_download_completed: hasAudio,\n    file_path: `/tmp/n8n_video_${originalData.id}_${originalData.media_type}_${originalData.slide}.mp4`,\n    audio_file_path: hasAudio ? `/tmp/n8n_audio_${originalData.id}_${originalData.media_type}_${originalData.slide}.mp4` : null\n  }\n}];"
      },
      "id": "MergeVideoAudioData",
      "name": "MergeVideoAudioData",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [600, 350]
    },



    {
      "parameters": {
        "jsCode": "// Build the process video command with resolved values\nconst item = $input.item;\nconsole.log('=== BUILDING PROCESS COMMAND ===');\nconsole.log('Input item:', JSON.stringify(item.json, null, 2));\n\nconst { id, media_type, slide, has_audio, audio_file_path } = item.json;\nconsole.log('Building process command for:', { id, media_type, slide, has_audio });\n\n// Validate required fields\nif (!id || !media_type || !slide) {\n  console.log('ERROR: Missing required fields in BuildProcessCommand');\n  console.log('id:', id);\n  console.log('media_type:', media_type);\n  console.log('slide:', slide);\n  throw new Error('Missing required fields in BuildProcessCommand');\n}\n\n// Build command based on whether we have audio\nlet bashCommand;\n\nif (has_audio && audio_file_path) {\n  console.log('Building FFmpeg command to merge video and audio');\n  bashCommand = `bash -c \"set -e; echo '=== DEBUG INFO ==='; echo 'Video file: /tmp/n8n_video_${id}_${media_type}_${slide}.mp4'; echo 'Audio file: /tmp/n8n_audio_${id}_${media_type}_${slide}.mp4'; echo 'Output file: /tmp/n8n_mux_${id}_${media_type}_${slide}.mp4'; echo '=== CHECKING FILES ==='; ls -la /tmp/ | grep n8n || echo 'No n8n files found'; if [ -f /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 ]; then echo 'Video file EXISTS'; ls -lh /tmp/n8n_video_${id}_${media_type}_${slide}.mp4; else echo 'Video file NOT FOUND'; fi; if [ -f /tmp/n8n_audio_${id}_${media_type}_${slide}.mp4 ]; then echo 'Audio file EXISTS'; ls -lh /tmp/n8n_audio_${id}_${media_type}_${slide}.mp4; else echo 'Audio file NOT FOUND'; fi; echo '=== ATTEMPTING AUDIO-VIDEO MERGE ==='; if [ ! -f /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 ]; then echo 'ERROR: Video file does not exist'; exit 1; fi; if [ ! -f /tmp/n8n_audio_${id}_${media_type}_${slide}.mp4 ]; then echo 'ERROR: Audio file does not exist'; exit 1; fi; if [ ! -s /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 ]; then echo 'ERROR: Video file is empty'; exit 1; fi; if [ ! -s /tmp/n8n_audio_${id}_${media_type}_${slide}.mp4 ]; then echo 'ERROR: Audio file is empty'; exit 1; fi; echo 'Both files found and not empty, merging with FFmpeg...'; ffmpeg -i /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 -i /tmp/n8n_audio_${id}_${media_type}_${slide}.mp4 -c:v copy -c:a aac -shortest /tmp/n8n_mux_${id}_${media_type}_${slide}.mp4; echo 'Audio-video merge completed successfully'\"`;\n} else {\n  console.log('No audio available, copying video only');\n  bashCommand = `bash -c \"set -e; echo '=== DEBUG INFO ==='; echo 'Video file: /tmp/n8n_video_${id}_${media_type}_${slide}.mp4'; echo 'Output file: /tmp/n8n_mux_${id}_${media_type}_${slide}.mp4'; echo '=== CHECKING VIDEO FILE ==='; ls -la /tmp/ | grep n8n || echo 'No n8n files found'; if [ -f /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 ]; then echo 'Video file EXISTS'; ls -lh /tmp/n8n_video_${id}_${media_type}_${slide}.mp4; else echo 'Video file NOT FOUND'; fi; echo '=== ATTEMPTING PROCESSING ==='; if [ ! -f /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 ]; then echo 'ERROR: Video file does not exist'; exit 1; fi; if [ ! -s /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 ]; then echo 'ERROR: Video file is empty'; exit 1; fi; echo 'Video file found and not empty, copying...'; cp /tmp/n8n_video_${id}_${media_type}_${slide}.mp4 /tmp/n8n_mux_${id}_${media_type}_${slide}.mp4; echo 'Video processing completed successfully'\"`;\n}\n\nconsole.log('Built bash command:', bashCommand);\n\n// Pass through the data with the resolved command\nreturn [{\n  json: {\n    id,\n    media_type,\n    slide,\n    chat_id: item.json.chat_id,\n    caption: item.json.caption,\n    media_url: item.json.media_url,\n    download_completed: item.json.download_completed,\n    file_path: item.json.file_path,\n    has_audio: item.json.has_audio,\n    audio_file_path: item.json.audio_file_path,\n    resolved_bash_command: bashCommand\n  }\n}];"
      },
      "id": "BuildProcessCommand",
      "name": "BuildProcessCommand",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [750, 300]
    },
    {
      "parameters": {
        "command": "={{$json.resolved_bash_command}}"
      },
      "id": "ProcessVideo",
      "name": "ProcessVideo",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [950, 360]
    },

    {
      "parameters": {
        "jsCode": "// Preserve processed video data after ProcessVideo execution\nconst item = $input.item;\nconsole.log('=== PRESERVE PROCESSED DATA ===');\nconsole.log('Input item:', JSON.stringify(item.json, null, 2));\n\n// Get the original data from the workflow context (before ProcessVideo)\nconst originalData = $('MergeVideoAudioData').item.json;\n\nif (!originalData) {\n  console.log('ERROR: Could not access original data from MergeVideoAudioData');\n  throw new Error('Could not access original data from MergeVideoAudioData');\n}\n\nconsole.log('Original data retrieved:', {\n  id: originalData.id,\n  media_type: originalData.media_type,\n  slide: originalData.slide,\n  chat_id: originalData.chat_id,\n  caption: originalData.caption,\n  media_url: originalData.media_url,\n  has_audio: originalData.has_audio,\n  audio_file_path: originalData.audio_file_path\n});\n\n// Pass through the data with all required fields preserved\nreturn [{\n  json: {\n    id: originalData.id,\n    media_type: originalData.media_type,\n    slide: originalData.slide,\n    chat_id: originalData.chat_id,\n    caption: originalData.caption,\n    media_url: originalData.media_url,\n    download_completed: originalData.download_completed,\n    file_path: originalData.file_path,\n    has_audio: originalData.has_audio,\n    audio_file_path: originalData.audio_file_path,\n    processing_completed: true,\n    mux_file_path: `/tmp/n8n_mux_${originalData.id}_${originalData.media_type}_${originalData.slide}.mp4`\n  }\n}];"
      },
      "id": "PreserveProcessedData",
      "name": "PreserveProcessedData",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [975, 360]
    },

    {
      "parameters": {
        "filePath": "=/tmp/n8n_mux_{{$json.id}}_{{$json.media_type}}_{{$json.slide}}.mp4",
        "binaryPropertyName": "data"
      },
      "id": "ReadMux",
      "name": "ReadMux",
      "type": "n8n-nodes-base.readBinaryFile",
      "typeVersion": 1,
      "position": [1000, 360]
    },

    {
      "parameters": {
        "jsCode": "// Memory-efficient data merging\nconst item = $input.item;\n\n// Validate input data\nif (!item || !item.json || !item.binary) {\n  throw new Error('Invalid input: Missing JSON or binary data');\n}\n\n// Only pass essential data to reduce memory usage\nreturn [{\n  json: {\n    id: item.json.id,\n    chat_id: item.json.chat_id,\n    caption: item.json.caption\n  },\n  binary: item.binary\n}];"
      },
      "id": "MergeVideoData",
      "name": "MergeVideoData",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1200, 360]
    },
    {
      "parameters": {
        "operation": "sendVideo",
        "chatId": "={{$json.chat_id}}",
        "binaryData": true,
        "additionalFields": {
          "caption": "={{$json.caption}}"
        }
      },
      "id": "SendVideo",
      "name": "SendVideo",
      "type": "n8n-nodes-base.telegram",
      "typeVersion": 1,
      "position": [1000, 300]
    },
    {
      "parameters": {
        "command": "rm -f /tmp/n8n_video_{{$json.id}}_{{$json.media_type}}_{{$json.slide}}.mp4 /tmp/n8n_audio_{{$json.id}}_{{$json.media_type}}_{{$json.slide}}.mp4 /tmp/n8n_mux_{{$json.id}}_{{$json.media_type}}_{{$json.slide}}.mp4"
      },
      "id": "Cleanup",
      "name": "Cleanup",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [1200, 300]
    },
    {
      "parameters": {
        "jsCode": "// LOOP ITERATION COUNTER: Track progress and continue loop if needed\nconst state = $getWorkflowStaticData('global');\nconst itemsToProcess = $node['Vars'].json['items_to_process'] || 5;\n\nconsole.log(`\\n📊 LOOP ITERATION COUNTER: Video processing completed`);\nconsole.log(`  Items processed so far: ${state.total_items_processed || 0}/${itemsToProcess}`);\nconsole.log(`  Progress: ${((state.total_items_processed || 0)/itemsToProcess*100).toFixed(1)}%`);\n\n// Check if we need to continue the loop\nif(state.total_items_processed < itemsToProcess && state.loop_items && state.loop_items.length > 0) {\n  console.log(`🔄 CONTINUING LOOP: ${state.loop_items.length} items remaining in queue`);\n  console.log(`  Next iteration will process: ${Math.min(itemsToProcess - state.total_items_processed, state.loop_items.length)} items`);\n  \n  // Return a signal to continue processing\n  return [{\n    json: {\n      continue_loop: true,\n      remaining_items: state.loop_items.length,\n      items_to_process: itemsToProcess,\n      total_processed: state.total_items_processed,\n      progress_percentage: ((state.total_items_processed || 0)/itemsToProcess*100).toFixed(1)\n    }\n  }];\n} else {\n  console.log(`✅ LOOP COMPLETE: All ${itemsToProcess} items have been processed`);\n  console.log(`  Final count: ${state.total_items_processed || 0} items processed`);\n  \n  // Reset loop state for next workflow run\n  state.loop_iteration = 0;\n  state.loop_start_time = Date.now();\n  state.total_items_processed = 0;\n  state.loop_items = [];\n  \n  return [{\n    json: {\n      continue_loop: false,\n      loop_complete: true,\n      total_processed: state.total_items_processed || 0,\n      final_message: `Loop completed successfully with ${state.total_items_processed || 0} items processed`\n    }\n  }];\n}"
      },
      "id": "LoopIterationCounter",
      "name": "LoopIterationCounter",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1400, 300]
    }
  ],
  "connections": {
    "CronTrigger": { "main": [[{ "node": "ManualTrigger", "type": "main", "index": 0 }]] },
    "ManualTrigger": { "main": [[{ "node": "Vars", "type": "main", "index": 0 }]] },
    "Vars": { "main": [[{ "node": "FetchJSON", "type": "main", "index": 0 }]] },
    "FetchJSON": { "main": [[{ "node": "ExtractMedia", "type": "main", "index": 0 }]] },
    "ExtractMedia": { "main": [[{ "node": "PersistentDeduplicate", "type": "main", "index": 0 }]] },
    "PersistentDeduplicate": { "main": [[{ "node": "Deduplicate", "type": "main", "index": 0 }]] },
    "Deduplicate": { "main": [[{ "node": "LoopControl", "type": "main", "index": 0 }]] },
    "LoopControl": { "main": [[{ "node": "CaptionBuilder", "type": "main", "index": 0 }]] },
    "CaptionBuilder": { "main": [[{ "node": "IfVideo", "type": "main", "index": 0 }]] },
    "IfVideo": {
      "main": [
        [{ "node": "SplitVideos", "type": "main", "index": 0 }],
        [{ "node": "SplitImages", "type": "main", "index": 0 }]
      ]
    },
    "SplitImages": { "main": [[{ "node": "ImageDeduplicate", "type": "main", "index": 0 }]] },
    "ImageDeduplicate": { "main": [[{ "node": "SendPhoto", "type": "main", "index": 0 }]] },
    "SendPhoto": { "main": [] },
    "SplitVideos": { "main": [[{ "node": "DebugDLVideo", "type": "main", "index": 0 }]] },
    "DebugDLVideo": { "main": [[{ "node": "SmartDownload", "type": "main", "index": 0 }]] },
    "SmartDownload": { "main": [[{ "node": "RedditAPI", "type": "main", "index": 0 }]] },
    "RedditAPI": { "main": [[{ "node": "ProcessRedditAPI", "type": "main", "index": 0 }]] },
    "ProcessRedditAPI": { "main": [[{ "node": "PostAPIDeduplicate", "type": "main", "index": 0 }]] },
    "PostAPIDeduplicate": { "main": [[{ "node": "BuildCurlCommand", "type": "main", "index": 0 }]] },
    "BuildCurlCommand": { "main": [[{ "node": "DLVideo", "type": "main", "index": 0 }]] },
    "DLVideo": { "main": [[{ "node": "PreserveVideoData", "type": "main", "index": 0 }]] },
    "PreserveVideoData": { "main": [[{ "node": "MergeVideoAudioData", "type": "main", "index": 0 }]] },
    "MergeVideoAudioData": { "main": [[{ "node": "BuildProcessCommand", "type": "main", "index": 0 }]] },
    "BuildProcessCommand": { "main": [[{ "node": "ProcessVideo", "type": "main", "index": 0 }]] },
    "ProcessVideo": { "main": [[{ "node": "PreserveProcessedData", "type": "main", "index": 0 }]] },
    "PreserveProcessedData": { "main": [[{ "node": "ReadMux", "type": "main", "index": 0 }]] },
    "ReadMux": { "main": [[{ "node": "MergeVideoData", "type": "main", "index": 0 }]] },
    "MergeVideoData": { "main": [[{ "node": "SendVideo", "type": "main", "index": 0 }]] },
    "SendVideo": { "main": [[{ "node": "Cleanup", "type": "main", "index": 0 }]] },
    "Cleanup": { "main": [[{ "node": "LoopIterationCounter", "type": "main", "index": 0 }]] }
  }
}
